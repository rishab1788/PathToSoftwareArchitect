# 📈 Spotting and Addressing Performance Problems

## 🔍 Identifying Performance Issues

**Every performance problem is the result of a queue building up somewhere:**
- Network Socket Queue
- Database I/O Queue
- OS Run Queue
- Etc.

### 🚨 Reasons for Queue Build-Up
- Inefficient or slow processing
- Serial resource access
- Limited resource capacity

## 🚀 Performance Principles

### 🛠 Efficiency
- **Efficient Resource Utilization**
  - I/O, Memory, Network, Disk
  - CPU
- **Efficient Logic**
  - Algorithms
  - Database queries
- **Efficient Data Storage**
  - Data structures
  - Database schema
- **Caching**
  - Cache frequently accessed data to reduce latency.

### 🕒 Concurrency
- **Hardware**
  - Multi-core processors
- **Software**
  - Queuing systems
  - Data coherence

### 📊 Capacity
- Ensure that the system can handle the expected load.

## 🎯 System Performance Objectives

- **Minimize Request-Response Latency**
  - Latency is measured in time units.
  - Depends on:
    - Wait/Idle Time
    - Processing Time

- **Maximize Throughput**
  - Throughput is measured as the rate of requests.
  - Depends on:
    - Latency
    - Capacity

⚠️ Throughput depends on latency; as architects, we must focus on reducing latency.

## 📏 Performance Measurement Metrics

- **Latency**
  - **Affects:** User experience
  - **Desired:** As low as possible
- **Throughput**
  - **Affects:** Number of users that can be supported
  - **Desired:** Greater than the request rate
- **Errors**
  - **Affects:** Functional correctness
  - **Desired:** None
- **Resource Saturation**
  - **Affects:** Hardware capacity required
  - **Desired:** Efficient utilization of all system resources

### 🔍 Tail Latency
- Tail latency indicates queuing of requests.
- It worsens with higher workloads.

⚠️ Average latency can hide the effects of tail latency. Measure 99th or 99.9th percentile latency for more accurate insights.

## 🌐 Network Latency

- **Factors:**
  - Data transfer (Global/Regional/Local Network)
  - TCP connections
  - SSL/TLS connections

### 🛠 Approaches

- **Server:**
  - Data format & compression
  - SSL session caching
  - Session/Data caching
  - Connection pool
  - RPC/gRPC protocol (minimal data transfer overhead, though interoperability might be hampered)

- **Browser:**
  - Persistent connection
  - Static data caching

## 🧠 Memory Latency

- **Factors:**
  - Finite Heap Memory
  - Large Heap Memory (processes occupying more memory than available, leading to usage of physical memory)
  - GC Algorithm (aggressive garbage collection when memory is low, affecting performance)
  - Finite buffer memory (Hard disk to memory operations)

### 🛠 Minimizing Memory Access Latency

- Avoid memory bloat (minimize heap space, streamline codebase)
- Use Weak/Soft References for large objects
- Utilize multiple smaller processes
- Choose an appropriate Garbage Collection Algorithm (e.g., batch processes, main process)
- Optimize buffer memory usage (compute over storage, database normalization)

## 💾 Disk Latency

- **Logging:**
  - Query optimization
  - Data caching
  - Random vs. Sequential Disk I/O (sequential is faster, ideal for logging)
  - Asynchronous logging
  - Sequential & Batch I/O

- **Web Content Files:**
  - Web content caching (e.g., reverse proxy)
  - Page cache, zero copy (retain files in memory, optimize kernel details)

- **Database Disk Access:**
  - Schema optimization
    - Denormalization vs. Normalization
    - Indexes (avoid full table scans)
  - Higher IOPS, RAID, SSD Disks
    - SSDs are faster but more expensive
    - Higher IOPS improves data access speed
    - RAID (data is replicated, enabling parallel reading and reducing overall latency)

# 🖥️ CPU Latency

## 🚩 Causes of CPU Latency
- **Inefficient Algorithms**: Poorly designed algorithms can lead to increased CPU time.
- **Context Switching**: When multiple threads are running, the CPU switches between them, which wastes time. For example, on a single CPU machine running two processes (T1 and T2), the CPU must restore the state from the process control block (PCB2), causing latency.

## 🛠️ Approaches to Minimize Context Switching

- **Efficient Algorithms & Queries**: Optimize algorithms and database queries to reduce CPU time.
- **Batch/Asynchronous I/O**: Combine write and read operations to minimize context switches.
- **Single-Threaded Model**: Utilize a single-threaded approach, as seen in technologies like JavaScript in Chrome, Node.js, and Nginx, to avoid the overhead of context switching.
- **Thread Pool Size**: Maintain the right number of threads to avoid unnecessary context switching.
- **Multiple Processes in Virtual Environments**: Run processes in isolated virtual environments or containers, each with a single process, to reduce the need for context switching.
