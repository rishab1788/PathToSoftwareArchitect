# ðŸŒ System Reliability

## ðŸš€ Horizontal Scaling & Distributed Systems

- **Availability**: Ensuring the system is always up and running.
- **Reliability**: The system performs as expected consistently.
- **Fault Tolerance**: The ability to continue operation despite failures.
- **Partial Failures**: Handling situations where parts of the system fail without affecting the whole.

## ðŸ› ï¸ Designing Fault Tolerance

### ðŸ”„ Redundancy
- **Hot**: Immediate failover with zero downtime.
- **Warm**: Ready to take over with minimal downtime.
- **Cold**: Needs manual intervention and time to take over.

### ðŸ©º Fault Detection
- **Health Checks**: Regularly verifying the systemâ€™s components are working properly.
- **Monitoring**: Continuous observation to detect issues early.

### â™»ï¸ Recovery
- **Stateless**: Easier to recover since no state is held within the service.
- **Stateful**: More complex recovery due to the need to maintain state.

## âš–ï¸ System Stability

### â³ Timeouts
- Avoid hanging requests by implementing timeouts.

### âš ï¸ Circuit Breakers
- Prevent cascading failures by stopping requests to failing services.

### ðŸƒâ€â™‚ï¸ Fail Fast & Load Shedding
- Quickly fail requests when a problem is detected to avoid overwhelming the system.

---

# ðŸ›‘ Failures in Large-Scale Systems

## ðŸŒ Characteristics of Large-Scale Systems
- **Distributed Systems**: Spread across multiple locations or machines.
  - **Large Number of Components**: Multiple services and machines involved.
  - **Independent Failures**: Components can fail independently.
- **Single Point of Failure**: Identifying and eliminating single points of failure is crucial.

## âš¡ Increased Risk of Partial Failures
- **Partial Failures**: Can lead to complete system failure if not handled correctly.

### ðŸ” Types of Partial Failures

#### ðŸŒ Network Failures
- LAN, WAN, Load Balancer issues.

#### ðŸ’» Machine Failures
- CPU, Disk, Memory malfunctions.

#### ðŸ› ï¸ Software Failures
- Process crashes or bugs.

#### ðŸŒªï¸ Disasters
- Data center outages.

#### ðŸ”§ Operational Failures
- Deployment errors, configuration mistakes, load-induced failures, external service issues.

---

## ðŸ’¡ Key Takeaways
- After a certain point, it's more economical to **recover** from failures than to **prevent** them entirely.
- **Hardware** and **networks** will inevitably fail, and software changes can introduce bugs.
- Disasters are unpredictable but inevitable.
- We should design systems that can **recover** from failures rather than attempting to prevent them entirely.


# ðŸš€ Reliability Engineering

### Core Concepts:
* **Reliability**, **Availability**, **Fault Tolerance**.

### ðŸ“ˆ Reliability
- A system is said to be reliable if it can continue to function correctly and remain available for operation, even in the presence of partial failures.
- ðŸ›©ï¸ **Example**: If an airplane loses one engine but still operates normally, it demonstrates reliability.
- Reliability is measured as the **probability of a system working correctly** over a given time interval.

### ðŸ•’ Availability
- Availability refers to the **probability of a system working correctly** at any given time and being available for operations.
- **Time-based Availability**: 
Availability = Uptime / (Uptime + Downtime)

 Availability = (Successful Requests / Total Requests) * 100%
- **Request-based Availability**: 

- ðŸ”„ Even if there's downtime, the system is expected to **recover quickly**.
- **High Availability**: It's difficult to achieve 100% availability, but systems like ATMs aim for 99.999% availability, which translates to just **5 minutes of downtime per year**.

#### Availability Requirements:
| Availability  | Max Disruption     | Application Categories                         |
| ------------- | ------------------ | ---------------------------------------------- |
| 99%           | 3 days 15 hours     | Batch Processing, Data Extraction              |
| 99.9%         | 8 hours 45 minutes  | Internal Tools (e.g., Knowledge, Project Tracking) |
| 99.95%        | 4 hours 22 minutes  | Online Commerce, Point of Sale                 |
| 99.99%        | 52 minutes          | Video Delivery, Broadcast Systems              |
| 99.999%       | 5 minutes           | ATM Transactions, Telecom Systems              |

ðŸ’¡ Systems should use permitted **downtimes** in their SLA/SLO for **planned rollouts** and updates.

---

## âš™ï¸ Fault Tolerance
- Fault tolerance is the **technique used to improve availability and/or reliability** of a system.
- It refers to a system's ability to automatically:
- ðŸ› ï¸ **Detect partial failures**
- ðŸ› ï¸ **Handle partial failures**
- ðŸ› ï¸ **Recover from partial failures**
- **Serviceability**: The ease with which a system can be serviced in the event of a failure also contributes to its availability.

---

## ðŸ›¡ï¸ Designing Fault Tolerance
- **Redundancy** â†’ **Fault Tolerance** â†’ **Recovery**

### ðŸ” Redundancy
- **Redundancy**: Duplication of critical components/functions to enhance reliability.
- Backup capacity is kept ready in case the primary system fails.

#### Types of Redundancy:

1. **ðŸŸ¢ Active Redundancy** (Hot Spare):
 - All nodes are active and process data simultaneously.
 - Ideal for providing **highest availability**.
 - **Example**: In load balancing, two systems handle 50% of the load. If one fails, the other takes over 100%.
 - **Advantage**: Very fast and efficient in ensuring availability.

2. **ðŸŸ¡ Passive Redundancy** (Warm Spare):
 - Only active nodes process the data.
 - Backup nodes are **ready to take over quickly** when needed.
 - **Example**: Similar to how substitute players are ready to jump in when one player is injured.

3. **ðŸ”´ Cold Redundancy** (Cold Spare):
 - Spare nodes are brought online only in the event of a failover.
 - Not ideal for **high availability** as it takes time to recover.

# ðŸš¨ Single Point of Failure (SPOF)

In any system, a **Single Point of Failure (SPOF)** refers to a component whose failure could lead to the failure of the entire system. Let's explore ways to mitigate SPOF risks through redundancy and fault tolerance.

---

## ðŸ–§ Key Components Leading to SPOF

### Load Balancer, Message Queue, Discovery Service
- If we don't provide **redundancy** for these components, they could become SPOFs.
- To ensure high availability, we need to implement redundancy for each.

---

## ðŸŸ¢ Redundancy for Stateless Components

- **Stateless components** are easier to scale by creating replicas.
- Replicas are deployed in an **Active-Active** manner for redundancy.
- **Examples**: Web applications, microservices.
- **Scaling**: We need to ensure we have enough replicas to handle the expected load.

---

## ðŸŸ¡ Redundancy for Stateful Components

- **Stateful components** such as databases, message queues, caches, and static content **require redundancy** to keep data in sync.
  
### ðŸ—„ï¸ Database Redundancy:
- **Master-Secondary Architecture**: 
  - [Inventory Service] âž¡ï¸ **Master DB** (syncs data) âž¡ï¸ **Secondary DB**.
  - This can be done via **synchronous** or **asynchronous replication**.
  - Active-Active setups are very fast, ensuring **high availability**.

### ðŸ“® Message Queue Redundancy:
- Redundancy for message queues typically follows a **Primary-Secondary** model.

### ðŸ—‚ï¸ Static Content Redundancy:
- **Handled by vendors**: Redundancy for static content (images, videos) is generally managed by third-party vendors (e.g., CDNs).
  
### ðŸ› ï¸ Cache Redundancy:
- **Cache Services** can be:
  - **Static or Dynamic** in nature.
  - Some cache services don't require redundancy (e.g., Memcached), as a cache miss is acceptable.
  - **Redis** provides built-in redundancy options.

---

## âš–ï¸ Load Balancer Redundancy

- A **Load Balancer** can be a **critical SPOF**. Hence, redundancy is crucial:
  - We set up **Primary** and **Secondary** load balancers to ensure seamless failover.

---

## ðŸ—ï¸ Infrastructure as SPOF

### ðŸ›œ Local Area Network (LAN):
- **Multiple Internet Connections**: Data centers often have multiple connections to the internet to prevent network failures from becoming SPOFs.

### ðŸ¢ Data Center Redundancy:
Data centers themselves can be vulnerable to failures due to natural disasters, such as floods or earthquakes. Therefore, it's essential to design for **data center redundancy**.

#### ðŸ›¡ï¸ Fault Isolation:
- Independent infrastructure across data centers ensures that issues in one center donâ€™t cascade to others.

#### ðŸ™ï¸ Zonal Redundancy:
- **High Availability** via an **Active-Active Setup**.
- Having two data centers at least **10 km apart** ensures that one can continue operations if the other fails.

#### ðŸŒ Regional Redundancy:
- **Disaster Recovery** through an **Active-Passive Setup**.
- The passive data center will take over only when the active one fails.

 
 

# ðŸ› ï¸ Fault Tolerant Design

In a distributed system, achieving fault tolerance is crucial to ensure the system continues to operate correctly even when partial failures occur. Letâ€™s break down how fault detection, monitoring, and health checks are applied.

---

## ðŸ” Fault Detection

Fault detection is about identifying when a component of the system is not functioning as expected. It is done by continuously monitoring the system's health and behavior.

---

## ðŸ“‰ Fault Models

Understanding different types of failures helps in designing effective fault-tolerant systems.

1. **Response Failure**: 
   - A server fails to receive or respond to incoming messages.
   - ðŸ›‘ **Symptom**: No response.

2. **Timeout Failure**: 
   - A serverâ€™s response takes longer than the expected timeout duration.
   - â³ **Symptom**: Response delay beyond allowed time.

3. **Incorrect Response Failure**:
   - A server responds with an incorrect result.
   - âš ï¸ **Symptom**: Incorrect data returned.

4. **Crash Failure**:
   - The server stops functioning entirely after working correctly for a period.
   - ðŸ’¥ **Symptom**: Complete halt of the service.

5. **Arbitrary Response Failure** (also called Byzantine failure):
   - A serverâ€™s response is incorrect due to compromised security or internal faults.
   - ðŸš¨ **Symptom**: Malicious or faulty data returned.

---

## ðŸ“Š Health Checks

**Health checks** are essential tools for monitoring the status and functionality of system components. They can detect failures and initiate recovery mechanisms.

### ðŸ”§ External Monitoring Service

- **Monitoring Service** constantly "pings" your servers:
  - ðŸ“ **Ping**: Sending periodic checks to confirm the systemâ€™s availability.
  - ðŸ›¡ï¸ **Goal**: Ensure services are reachable and functioning properly.

### ðŸ’“ Internal Cluster Monitoring

- **Heartbeats**: Periodic signals exchanged between nodes in a redundancy cluster to ensure they're functioning properly.
  - Useful for **stateful** and **clustered** components to detect partial or complete failures.

---

## ðŸš¨ External Monitoring Service

An external health check service monitors the system and responds accordingly:

### ðŸ›Žï¸ Health Check Service Generates:

1. **Alerts**: 
   - ðŸ› ï¸ Used to notify administrators or systems of issues that need recovery.
   
2. **Events**:
   - ðŸ“ˆ Triggered to handle system scaling based on monitored performance.
   
### ðŸš¥ Application Health Checks

Health checks monitor different parameters of a service:

1. **HTTP Response**: 
   - ðŸ–¥ï¸ Ensures that web services are responding correctly.
   
2. **TCP Response**: 
   - ðŸ”Œ Monitors server connectivity through established protocols.

### â±ï¸ Periodic Health Checks

Performed regularly to keep track of the system's health:

- **Response Code**: Expected HTTP status codes (e.g., 200 OK).
- **Response Time**: Monitor latency and ensure it is within acceptable limits.
- **Number of Retries**: 
  - If multiple retries are needed to get a response, it could be a sign of service degradation.
  - âš ï¸ Status: **UP** (service running smoothly) or **DOWN** (service unavailable).

**Public Cloud** makes it easy to implement periodic health checks.

---

## ðŸ§ What Happens if the Monitoring Service Fails?

When the **monitoring service** itself is down, it poses a significant challenge. One solution is to have a **Health Check Service** to monitor the monitoring service itself!

---

## ðŸ¤ Inter-Cluster Monitoring

- **Heartbeat Exchange**: Periodic heartbeats are exchanged between nodes in a redundancy cluster to ensure all nodes are functioning.
- **Communication Protocols** are necessary for recovering from failures.
- **Useful for Stateful Components**: 
  - In **stateful** systems, the cluster needs to coordinate closely for fault detection and recovery.

---

## ðŸš¦ Fault Detection Monitoring Techniques

1. **Health Checks**:
   - Commonly used in stateless services.
   - ðŸŽ¯ Focus: Check service health at predefined intervals.

2. **Heartbeat Monitoring**:
   - Used in cluster-based systems where nodes monitor each other.
   - ðŸ’“ Focus: Ensure that nodes remain active and synchronize state.



Stateless Recovery - 

Can use existing scalability mechanism for recovery 
Hot standby 
  * Have active redundant instances up and running.
Warm standby
  * Bring up new instances as and when needed.
  * Terminate unhealthy instances if not dead already.
  * Launch a new instance.

    [Load Balancer ] ----> 1,2,3, ---> Monitoring and Health check () --> autoscaler -> VM container/image.

Stateful failover

    Virtual IP                   
  client-> DNS(10.2.2.2)
      floatingIP
            |
 primary -HeartBeat-   standby
(192.1.1.1) (192.2.2.2)

Registry/Router/DNS
      Client --> Registery --> Instance 2
       |         
    instance1

 
      
